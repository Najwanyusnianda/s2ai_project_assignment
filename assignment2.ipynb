{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Candidate Pairs of Acronyms and Expansions (Assignment 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "#from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 extract dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(\"dataacro.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(\"dataacro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Define extract dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(list_doc):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for i in range(len(list_doc)):\n",
    "        lines_feature=list_doc[i].split(\" \")[-8:len(list_doc[i])]\n",
    "        line_label=list_doc[i].split(\" \")[-9]\n",
    "        list_features=[float(line.strip().split(\":\")[1]) for line in lines_feature]\n",
    "        list_label=int(line_label) \n",
    "        X.append(list_features)\n",
    "        y.append(list_label)\n",
    "    \n",
    "    return X,y  \n",
    "\n",
    "def extract_text(list_doc):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for line in list_doc:\n",
    "        text_feature=line.split(\" \")[:-9]\n",
    "        text_feature=\" \".join(text_feature)\n",
    "        line_label=line.split(\" \")[-9]\n",
    "        \n",
    "        X.append(text_feature)\n",
    "        y.append(line_label)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Load and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length:  4000\n",
      "Training data text length:  4000\n",
      "Training label length:  4000\n",
      "Testing data length:  1099\n",
      "Testing data text length:  1099\n",
      "Testing label length:  1099\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(\"dataacro/trainingset.txt\", \"r\") as file:\n",
    "    training_lines = file.readlines()\n",
    "\n",
    "with open(\"dataacro/testingset.txt\", \"r\") as file:\n",
    "    testing_lines = file.readlines()\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "X_train,y_train=extract_feature(training_lines)\n",
    "X_test,y_test=extract_feature(testing_lines)   \n",
    "\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "\n",
    "\n",
    "X_test=np.array(X_test)\n",
    "y_test=np.array(y_test)\n",
    "\n",
    "#replace -1 with 0\n",
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "y_test = np.where(y_test == -1, 0, y_test)\n",
    "\n",
    "\n",
    "\n",
    "X_train_text= extract_text(training_lines)\n",
    "X_train_text=np.array(X_train_text)\n",
    "X_train_text=X_train_text.reshape(-1,1)\n",
    "X_test_text=extract_text(testing_lines)\n",
    "X_test_text=np.array(X_test_text)\n",
    "X_test_text=X_test_text.reshape(-1,1)\n",
    "#check data length\n",
    "print(\"Training data length: \", len(X_train))\n",
    "print(\"Training data text length: \", len(X_train_text))\n",
    "print(\"Training label length: \", len(y_train))\n",
    "print(\"Testing data length: \", len(X_test))\n",
    "print(\"Testing data text length: \", len(X_test_text))\n",
    "print(\"Testing label length: \", len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Describe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |       F1 |       F2 |        F3 |   F4 |   F5 |       F6 |   F7 |        F8 |   label |\n",
      "|---:|---------:|---------:|----------:|-----:|-----:|---------:|-----:|----------:|--------:|\n",
      "|  0 | 0.918296 | 1        | -0.666667 |  0   | 1    | 0.5      |    0 | 0.39309   |       0 |\n",
      "|  1 | 1        | 0.5      | -2        |  0   | 0.75 | 0        |    0 | 0.0357143 |       0 |\n",
      "|  2 | 0.970951 | 1        | -1        |  0.5 | 1    | 0.333333 |    0 | 0.400612  |       0 |\n",
      "|  3 | 1        | 0.75     | -2        |  0   | 1    | 1        |    1 | 0.392857  |       0 |\n",
      "|  4 | 0.970951 | 0.666667 | -2.5      |  0   | 1    | 0        |    0 | 0.0196596 |       0 |\n"
     ]
    }
   ],
   "source": [
    "col_name=['F'+str(i+1) for i in range(X_train.shape[1])]\n",
    "df_train=pd.concat([pd.DataFrame(X_train,columns=col_name),pd.DataFrame(y_train,columns=['label'])],axis=1)\n",
    "\n",
    "print(df_train.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Fitur text                       |   label |\n",
      "|---:|:---------------------------------|--------:|\n",
      "|  0 | BUMD=>Usaha Milik                |       0 |\n",
      "|  1 | TNI=>meminjam senjata dari oknum |       0 |\n",
      "|  2 | PKI=>Panitia Pengawas            |       0 |\n",
      "|  3 | MA=>putusan Mahkamah             |       0 |\n",
      "|  4 | TI=>com Mati body                |       0 |\n"
     ]
    }
   ],
   "source": [
    "col_name=['Fitur text']\n",
    "df_train_text=pd.concat([pd.DataFrame(X_train_text,columns=col_name),pd.DataFrame(y_train,columns=['label'])],axis=1)\n",
    "\n",
    "print(df_train_text.head().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Model - Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model_df_training=pd.DataFrame()\n",
    "eval_model_df_testing=pd.DataFrame()\n",
    "eval_model_time=pd.DataFrame()\n",
    "\n",
    "def input_model_result(df,y_true,y_pred,model_name):\n",
    "    cm=confusion_matrix(y_true,y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    pre=precision_score(y_true,y_pred)\n",
    "    rec=recall_score(y_true,y_pred)\n",
    "    f1=f1_score(y_true,y_pred)\n",
    "    \n",
    "    data={\n",
    "            'Model':model_name,\n",
    "            \"Confusion Matrix\": cm.tolist(),\n",
    "            \"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn,# Convert to list to avoid issues\n",
    "            \"Precision\": pre,\n",
    "            \"Recall\": rec,\n",
    "            \"F1-Score\": f1,\n",
    "\n",
    "        }\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame([data])\n",
    "    else:\n",
    "\n",
    "        if model_name in df['Model'].values:\n",
    "            print(\"Model already exists in the dataframe\")\n",
    "        else:\n",
    "            df = pd.concat([df, pd.DataFrame([data])], ignore_index=True)\n",
    "        \n",
    "            print(f\"added new result for model: {model_name} \")\n",
    "    return df\n",
    "\n",
    "def input_train_test_time(df,tr_time,pred_time,model_name):\n",
    "        data={\n",
    "            'Model':model_name,\n",
    "            \"Training_time\":tr_time,\n",
    "            \"Predict_time\":pred_time\n",
    "        }\n",
    "        if df.empty:\n",
    "            df = pd.DataFrame([data])\n",
    "        else:\n",
    "\n",
    "            if model_name in df['Model'].values:\n",
    "                print(\"Model already exists in the dataframe\")\n",
    "            else:\n",
    "                df = pd.concat([df, pd.DataFrame([data])], ignore_index=True)\n",
    "            \n",
    "                print(f\"added new time_result for model: {model_name} \")\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 9.7246 seconds\n",
      "The best parameters are {'C': 500.0, 'degree': 2, 'gamma': 0.2, 'kernel': 'poly'} witha score of 0.99\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(random_state=0)\n",
    "\n",
    "x = [100.0,500.0]\n",
    "y = [0.1,0.2,0.3]\n",
    "z = [2,3]    \n",
    "parameters=[#{'C': x,'kernel': ['linear']},\n",
    "            #{'C': x,'kernel': ['rbf'],'gamma': y} ,\n",
    "            {'C': x,'kernel': ['poly'],'gamma': y,'degree': z}\n",
    "           ]\n",
    "grid=GridSearchCV(estimator = svm,\n",
    "                        param_grid = parameters,\n",
    "                        scoring='f1',\n",
    "                        cv=10,\n",
    "                        n_jobs=-1)\n",
    "start_tm=time.time()\n",
    "grid=grid.fit(X_train,y_train)\n",
    "finish_tm=time.time()\n",
    "training_time = finish_tm - start_tm\n",
    "svm= grid.best_estimator_\n",
    "best_params=grid.best_params_\n",
    "best_score=grid.best_score_\n",
    "\n",
    "\n",
    "print(f\"Training Time: {training_time:.4f} seconds\")\n",
    "print(f\"The best parameters are {grid.best_params_} with\" +\n",
    "          f\"a score of {grid.best_score_:.2f}\")\n",
    "\n",
    "#The best parameters are {'C': 500.0, 'degree': 2, 'gamma': 0.2, 'kernel': 'poly'} witha score of 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added new result for model: SVM  \n",
      "added new result for model: SVM  \n",
      "added new time_result for model: SVM  \n"
     ]
    }
   ],
   "source": [
    "model_name='SVM '\n",
    "\n",
    "start_tm=time.time()\n",
    "y_pred_train= svm.predict(X_train)\n",
    "finish_tm=time.time()\n",
    "pred_train_time=finish_tm - start_tm\n",
    "\n",
    "\n",
    "y_pred_test= svm.predict(X_test)\n",
    "\n",
    "#input to eval table\n",
    "eval_model_df_testing = input_model_result(eval_model_df_testing, y_test, y_pred_test,model_name=model_name)\n",
    "eval_model_df_training= input_model_result(eval_model_df_training, y_train, y_pred_train,model_name=model_name)\n",
    "eval_model_time=input_train_test_time(eval_model_time,training_time,pred_train_time,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector Machine (SVM)</td>\n",
       "      <td>[[1961, 39], [13, 1987]]</td>\n",
       "      <td>1987</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.98075</td>\n",
       "      <td>0.9935</td>\n",
       "      <td>0.987084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model          Confusion Matrix    TP  FP  FN    TN  \\\n",
       "0  Support Vector Machine (SVM)  [[1961, 39], [13, 1987]]  1987  39  13  1961   \n",
       "\n",
       "   Precision  Recall  F1-Score  \n",
       "0    0.98075  0.9935  0.987084  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model_df_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 KNN Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 1.2790 seconds\n",
      "The best parameters are {'n_neighbors': np.int64(9)} with f1-score of 0.71\n"
     ]
    }
   ],
   "source": [
    "## build model\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = {\"n_neighbors\": np.arange(2, 10)}\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(knn, param_grid=param_grid,scoring=\"f1\",cv=cv)\n",
    "\n",
    "start_tm=time.time()\n",
    "grid=grid.fit(X_train,y_train)\n",
    "finish_tm=time.time()\n",
    "training_time = finish_tm - start_tm\n",
    "\n",
    "print(f\"Training Time: {training_time:.4f} seconds\")\n",
    "print(f\"The best parameters are {grid.best_params_} with\" +\n",
    "          f\" f1-score of {grid.best_score_:.2f}\")\n",
    "\n",
    "knn = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='K-Nearest Neighbor (kNN) '\n",
    "\n",
    "start_tm=time.time()\n",
    "y_pred_train= knn.predict(X_train)\n",
    "finish_tm=time.time()\n",
    "pred_train_time=finish_tm - start_tm\n",
    "\n",
    "\n",
    "y_pred_test= knn.predict(X_test)\n",
    "\n",
    "#input to eval table\n",
    "eval_model_df_testing = input_model_result(eval_model_df_testing, y_test, y_pred_test,model_name=model_name)\n",
    "eval_model_df_training= input_model_result(eval_model_df_training, y_train, y_pred_train,model_name=model_name)\n",
    "eval_model_time=input_train_test_time(eval_model_time,training_time,pred_train_time,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-Nearest Neighbor (kNN)</td>\n",
       "      <td>[[1967, 33], [1232, 768]]</td>\n",
       "      <td>768</td>\n",
       "      <td>33</td>\n",
       "      <td>1232</td>\n",
       "      <td>1967</td>\n",
       "      <td>0.958801</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.548376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model           Confusion Matrix   TP  FP    FN    TN  \\\n",
       "0  K-Nearest Neighbor (kNN)   [[1967, 33], [1232, 768]]  768  33  1232  1967   \n",
       "\n",
       "   Precision  Recall  F1-Score  \n",
       "0   0.958801   0.384  0.548376  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model_df_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 0.0020 seconds\n"
     ]
    }
   ],
   "source": [
    "nb=GaussianNB()\n",
    "start_tm=time.time()\n",
    "nb.fit(X_train,y_train)\n",
    "finish_tm=time.time()\n",
    "training_time = finish_tm - start_tm\n",
    "\n",
    "print(f\"Training Time: {training_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added new result for model: Naive Bayes  \n",
      "added new result for model: Naive Bayes  \n",
      "added new time_result for model: Naive Bayes  \n"
     ]
    }
   ],
   "source": [
    "model_name='Naive Bayes '\n",
    "\n",
    "start_tm=time.time()\n",
    "y_pred_train= nb.predict(X_train)\n",
    "finish_tm=time.time()\n",
    "pred_train_time=finish_tm - start_tm\n",
    "\n",
    "\n",
    "y_pred_test= nb.predict(X_test)\n",
    "\n",
    "#input to eval table\n",
    "eval_model_df_testing = input_model_result(eval_model_df_testing, y_test, y_pred_test,model_name=model_name)\n",
    "eval_model_df_training= input_model_result(eval_model_df_training, y_train, y_pred_train,model_name=model_name)\n",
    "eval_model_time=input_train_test_time(eval_model_time,training_time,pred_train_time,model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 0.3248 seconds\n",
      "The best parameters are {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2} with f1-score of 0.99\n"
     ]
    }
   ],
   "source": [
    "dt=tree.DecisionTreeClassifier()\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 10, None],  \n",
    "    \"min_samples_split\": [2, 5, 10],  \n",
    "    \"min_samples_leaf\": [1, 2, 5],  \n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(dt, param_grid, cv=5, scoring=\"f1\", n_jobs=-1)\n",
    "start_tm=time.time()\n",
    "grid=grid.fit(X_train,y_train)\n",
    "finish_tm=time.time()\n",
    "training_time = finish_tm - start_tm\n",
    "dt = grid.best_estimator_\n",
    "\n",
    "print(f\"Training Time: {training_time:.4f} seconds\")\n",
    "print(f\"The best parameters are {grid.best_params_} with\" +\n",
    "          f\" f1-score of {grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added new result for model: Decision Tree  \n",
      "added new result for model: Decision Tree  \n",
      "added new time_result for model: Decision Tree  \n"
     ]
    }
   ],
   "source": [
    "model_name='Decision Tree '\n",
    "\n",
    "start_tm=time.time()\n",
    "y_pred_train= dt.predict(X_train)\n",
    "finish_tm=time.time()\n",
    "pred_train_time=finish_tm - start_tm\n",
    "\n",
    "\n",
    "y_pred_test= dt.predict(X_test)\n",
    "\n",
    "#input to eval table\n",
    "eval_model_df_testing = input_model_result(eval_model_df_testing, y_test, y_pred_test,model_name=model_name)\n",
    "eval_model_df_training= input_model_result(eval_model_df_training, y_train, y_pred_train,model_name=model_name)\n",
    "eval_model_time=input_train_test_time(eval_model_time,training_time,pred_train_time,model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Model - BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Penyiapan Dataset dan Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 32])\n",
      "torch.Size([4000, 32])\n",
      "{'input_ids': [101, 26352, 2094, 1027, 1028, 3915, 3270, 23689, 5480, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of lists and join them into a list of strings\n",
    "X_train_flat = [\" \".join(item) for item in X_train_text]\n",
    "X_test_flat = [\" \".join(item) for item in X_test_text]\n",
    "y_train_t=torch.tensor(y_train)\n",
    "y_test_t=torch.tensor(y_test)\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text data, ensure padding and truncation for uniform length\n",
    "tokenized_data = tokenizer(X_train_flat, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_test_data = tokenizer(X_test_flat, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Inspect tokenized data (input_ids and attention_mask)\n",
    "print(tokenized_data['input_ids'].shape)\n",
    "print(tokenized_data['attention_mask'].shape)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': tokenized_data['input_ids'],\n",
    "    'attention_mask': tokenized_data['attention_mask'],\n",
    "    'labels': y_train_t\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'input_ids': tokenized_test_data['input_ids'],\n",
    "    'attention_mask': tokenized_test_data['attention_mask'],\n",
    "    'labels': y_test_t  # Labels for test data\n",
    "})\n",
    "\n",
    "# Verify the structure\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Inisiasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Penyiapan Trainer dan Melatih Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Najwan\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Najwan\\AppData\\Local\\Temp\\ipykernel_11920\\1794867259.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  bert = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d46d9e665e944b288a0d2270e81bdad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7118, 'grad_norm': 3.385584592819214, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.02}\n",
      "{'loss': 0.752, 'grad_norm': 8.507855415344238, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.04}\n",
      "{'loss': 0.7197, 'grad_norm': 8.07103443145752, 'learning_rate': 3e-06, 'epoch': 0.06}\n",
      "{'loss': 0.6886, 'grad_norm': 3.3912525177001953, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.08}\n",
      "{'loss': 0.6719, 'grad_norm': 4.134318828582764, 'learning_rate': 5e-06, 'epoch': 0.1}\n",
      "{'loss': 0.7032, 'grad_norm': 3.7122936248779297, 'learning_rate': 6e-06, 'epoch': 0.12}\n",
      "{'loss': 0.6939, 'grad_norm': 4.461877346038818, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.14}\n",
      "{'loss': 0.6985, 'grad_norm': 7.539129734039307, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.16}\n",
      "{'loss': 0.6743, 'grad_norm': 8.642992973327637, 'learning_rate': 9e-06, 'epoch': 0.18}\n",
      "{'loss': 0.6601, 'grad_norm': 8.059813499450684, 'learning_rate': 1e-05, 'epoch': 0.2}\n",
      "{'loss': 0.6861, 'grad_norm': 8.516935348510742, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 0.6719, 'grad_norm': 8.415019989013672, 'learning_rate': 1.2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.6257, 'grad_norm': 7.575857639312744, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6334, 'grad_norm': 16.292251586914062, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.6195, 'grad_norm': 12.092585563659668, 'learning_rate': 1.5e-05, 'epoch': 0.3}\n",
      "{'loss': 0.5898, 'grad_norm': 6.847112655639648, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.32}\n",
      "{'loss': 0.5257, 'grad_norm': 14.118123054504395, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.34}\n",
      "{'loss': 0.5718, 'grad_norm': 10.33405876159668, 'learning_rate': 1.8e-05, 'epoch': 0.36}\n",
      "{'loss': 0.4953, 'grad_norm': 12.318638801574707, 'learning_rate': 1.9e-05, 'epoch': 0.38}\n",
      "{'loss': 0.4174, 'grad_norm': 30.69890594482422, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5524, 'grad_norm': 14.105093002319336, 'learning_rate': 2.1e-05, 'epoch': 0.42}\n",
      "{'loss': 0.4977, 'grad_norm': 7.217209815979004, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.44}\n",
      "{'loss': 0.4422, 'grad_norm': 18.8013973236084, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.46}\n",
      "{'loss': 0.616, 'grad_norm': 15.970783233642578, 'learning_rate': 2.4e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3967, 'grad_norm': 22.311511993408203, 'learning_rate': 2.5e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5757, 'grad_norm': 17.057191848754883, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3282, 'grad_norm': 9.36546802520752, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3721, 'grad_norm': 34.01588821411133, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.56}\n",
      "{'loss': 0.4188, 'grad_norm': 77.64092254638672, 'learning_rate': 2.9e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3995, 'grad_norm': 42.88864517211914, 'learning_rate': 3e-05, 'epoch': 0.6}\n",
      "{'loss': 0.4345, 'grad_norm': 5.370146751403809, 'learning_rate': 3.1e-05, 'epoch': 0.62}\n",
      "{'loss': 0.4179, 'grad_norm': 6.455511569976807, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3461, 'grad_norm': 11.271000862121582, 'learning_rate': 3.3e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3439, 'grad_norm': 8.42058277130127, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.68}\n",
      "{'loss': 0.2941, 'grad_norm': 5.2973809242248535, 'learning_rate': 3.5e-05, 'epoch': 0.7}\n",
      "{'loss': 0.191, 'grad_norm': 24.450651168823242, 'learning_rate': 3.6e-05, 'epoch': 0.72}\n",
      "{'loss': 0.4615, 'grad_norm': 21.462032318115234, 'learning_rate': 3.7e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3071, 'grad_norm': 54.99936294555664, 'learning_rate': 3.8e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6785, 'grad_norm': 2.03426194190979, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.78}\n",
      "{'loss': 0.43, 'grad_norm': 33.05990982055664, 'learning_rate': 4e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5213, 'grad_norm': 21.138689041137695, 'learning_rate': 4.1e-05, 'epoch': 0.82}\n",
      "{'loss': 0.4016, 'grad_norm': 13.072937965393066, 'learning_rate': 4.2e-05, 'epoch': 0.84}\n",
      "{'loss': 0.2762, 'grad_norm': 2.1924808025360107, 'learning_rate': 4.3e-05, 'epoch': 0.86}\n",
      "{'loss': 0.2188, 'grad_norm': 7.110755920410156, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.88}\n",
      "{'loss': 0.4288, 'grad_norm': 4.122611999511719, 'learning_rate': 4.5e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3749, 'grad_norm': 31.897445678710938, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.92}\n",
      "{'loss': 0.1574, 'grad_norm': 0.790407657623291, 'learning_rate': 4.7e-05, 'epoch': 0.94}\n",
      "{'loss': 0.2676, 'grad_norm': 5.2163896560668945, 'learning_rate': 4.8e-05, 'epoch': 0.96}\n",
      "{'loss': 0.1979, 'grad_norm': 1.0362743139266968, 'learning_rate': 4.9e-05, 'epoch': 0.98}\n",
      "{'loss': 0.292, 'grad_norm': 0.46715211868286133, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9e62cf240e4238a25a9844440d2ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.027740716934204, 'eval_runtime': 9.0126, 'eval_samples_per_second': 121.94, 'eval_steps_per_second': 15.312, 'epoch': 1.0}\n",
      "{'loss': 0.1879, 'grad_norm': 87.7505111694336, 'learning_rate': 4.9500000000000004e-05, 'epoch': 1.02}\n",
      "{'loss': 0.1181, 'grad_norm': 0.1850026398897171, 'learning_rate': 4.9e-05, 'epoch': 1.04}\n",
      "{'loss': 0.3287, 'grad_norm': 12.74843978881836, 'learning_rate': 4.85e-05, 'epoch': 1.06}\n",
      "{'loss': 0.2757, 'grad_norm': 8.986218452453613, 'learning_rate': 4.8e-05, 'epoch': 1.08}\n",
      "{'loss': 0.174, 'grad_norm': 3.8822269439697266, 'learning_rate': 4.75e-05, 'epoch': 1.1}\n",
      "{'loss': 0.2596, 'grad_norm': 0.1272934079170227, 'learning_rate': 4.7e-05, 'epoch': 1.12}\n",
      "{'loss': 0.5462, 'grad_norm': 2.107450008392334, 'learning_rate': 4.6500000000000005e-05, 'epoch': 1.14}\n",
      "{'loss': 0.1115, 'grad_norm': 0.24286557734012604, 'learning_rate': 4.600000000000001e-05, 'epoch': 1.16}\n",
      "{'loss': 0.3441, 'grad_norm': 18.463790893554688, 'learning_rate': 4.55e-05, 'epoch': 1.18}\n",
      "{'loss': 0.1749, 'grad_norm': 0.14933286607265472, 'learning_rate': 4.5e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2486, 'grad_norm': 0.05156250670552254, 'learning_rate': 4.4500000000000004e-05, 'epoch': 1.22}\n",
      "{'loss': 0.2997, 'grad_norm': 0.10728790611028671, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.24}\n",
      "{'loss': 0.1881, 'grad_norm': 43.778385162353516, 'learning_rate': 4.35e-05, 'epoch': 1.26}\n",
      "{'loss': 0.1624, 'grad_norm': 74.79105377197266, 'learning_rate': 4.3e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1501, 'grad_norm': 0.8505126237869263, 'learning_rate': 4.25e-05, 'epoch': 1.3}\n",
      "{'loss': 0.282, 'grad_norm': 0.06988612562417984, 'learning_rate': 4.2e-05, 'epoch': 1.32}\n",
      "{'loss': 0.2693, 'grad_norm': 0.08893749862909317, 'learning_rate': 4.15e-05, 'epoch': 1.34}\n",
      "{'loss': 0.3364, 'grad_norm': 23.815759658813477, 'learning_rate': 4.1e-05, 'epoch': 1.36}\n",
      "{'loss': 0.2946, 'grad_norm': 20.54019546508789, 'learning_rate': 4.05e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0816, 'grad_norm': 1.0866503715515137, 'learning_rate': 4e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0863, 'grad_norm': 42.27649688720703, 'learning_rate': 3.9500000000000005e-05, 'epoch': 1.42}\n",
      "{'loss': 0.1662, 'grad_norm': 28.13890838623047, 'learning_rate': 3.9000000000000006e-05, 'epoch': 1.44}\n",
      "{'loss': 0.1341, 'grad_norm': 12.461310386657715, 'learning_rate': 3.85e-05, 'epoch': 1.46}\n",
      "{'loss': 0.2496, 'grad_norm': 0.02673514559864998, 'learning_rate': 3.8e-05, 'epoch': 1.48}\n",
      "{'loss': 0.4017, 'grad_norm': 42.72496795654297, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0951, 'grad_norm': 29.85438346862793, 'learning_rate': 3.7e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0283, 'grad_norm': 0.1962345987558365, 'learning_rate': 3.65e-05, 'epoch': 1.54}\n",
      "{'loss': 0.3337, 'grad_norm': 2.702988386154175, 'learning_rate': 3.6e-05, 'epoch': 1.56}\n",
      "{'loss': 0.179, 'grad_norm': 0.07957228273153305, 'learning_rate': 3.55e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0106, 'grad_norm': 0.171937495470047, 'learning_rate': 3.5e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2827, 'grad_norm': 0.11179587244987488, 'learning_rate': 3.45e-05, 'epoch': 1.62}\n",
      "{'loss': 0.3364, 'grad_norm': 8.301400184631348, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.64}\n",
      "{'loss': 0.1992, 'grad_norm': 0.33499011397361755, 'learning_rate': 3.35e-05, 'epoch': 1.66}\n",
      "{'loss': 0.1057, 'grad_norm': 0.18943852186203003, 'learning_rate': 3.3e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0187, 'grad_norm': 0.07008541375398636, 'learning_rate': 3.2500000000000004e-05, 'epoch': 1.7}\n",
      "{'loss': 0.2025, 'grad_norm': 1.642625331878662, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0769, 'grad_norm': 0.308072030544281, 'learning_rate': 3.15e-05, 'epoch': 1.74}\n",
      "{'loss': 0.2423, 'grad_norm': 0.060294654220342636, 'learning_rate': 3.1e-05, 'epoch': 1.76}\n",
      "{'loss': 0.121, 'grad_norm': 18.164997100830078, 'learning_rate': 3.05e-05, 'epoch': 1.78}\n",
      "{'loss': 0.373, 'grad_norm': 0.3064994513988495, 'learning_rate': 3e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0473, 'grad_norm': 0.1543024182319641, 'learning_rate': 2.95e-05, 'epoch': 1.82}\n",
      "{'loss': 0.2153, 'grad_norm': 0.08156055957078934, 'learning_rate': 2.9e-05, 'epoch': 1.84}\n",
      "{'loss': 0.0416, 'grad_norm': 68.61300659179688, 'learning_rate': 2.8499999999999998e-05, 'epoch': 1.86}\n",
      "{'loss': 0.2453, 'grad_norm': 0.06490118056535721, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.88}\n",
      "{'loss': 0.1651, 'grad_norm': 0.15159697830677032, 'learning_rate': 2.7500000000000004e-05, 'epoch': 1.9}\n",
      "{'loss': 0.2635, 'grad_norm': 0.08003723621368408, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0317, 'grad_norm': 0.12692829966545105, 'learning_rate': 2.6500000000000004e-05, 'epoch': 1.94}\n",
      "{'loss': 0.105, 'grad_norm': 0.08461615443229675, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0529, 'grad_norm': 30.485170364379883, 'learning_rate': 2.5500000000000003e-05, 'epoch': 1.98}\n",
      "{'loss': 0.2497, 'grad_norm': 12.506335258483887, 'learning_rate': 2.5e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fda8a23353849ce979c241bed6bfd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9373782873153687, 'eval_runtime': 8.9958, 'eval_samples_per_second': 122.168, 'eval_steps_per_second': 15.34, 'epoch': 2.0}\n",
      "{'loss': 0.0645, 'grad_norm': 0.023603620007634163, 'learning_rate': 2.45e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0746, 'grad_norm': 1.3115837574005127, 'learning_rate': 2.4e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0013, 'grad_norm': 0.176570862531662, 'learning_rate': 2.35e-05, 'epoch': 2.06}\n",
      "{'loss': 0.1281, 'grad_norm': 13.701517105102539, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0546, 'grad_norm': 10.838424682617188, 'learning_rate': 2.25e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0989, 'grad_norm': 191.8440399169922, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0018, 'grad_norm': 0.1621282398700714, 'learning_rate': 2.15e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0712, 'grad_norm': 7.102508544921875, 'learning_rate': 2.1e-05, 'epoch': 2.16}\n",
      "{'loss': 0.1159, 'grad_norm': 0.2157527655363083, 'learning_rate': 2.05e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0641, 'grad_norm': 0.055868420749902725, 'learning_rate': 2e-05, 'epoch': 2.2}\n",
      "{'loss': 0.002, 'grad_norm': 0.02987760491669178, 'learning_rate': 1.9500000000000003e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0634, 'grad_norm': 0.0247687716037035, 'learning_rate': 1.9e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0066, 'grad_norm': 0.020097684115171432, 'learning_rate': 1.85e-05, 'epoch': 2.26}\n",
      "{'loss': 0.1579, 'grad_norm': 0.01939508505165577, 'learning_rate': 1.8e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0556, 'grad_norm': 0.03947187587618828, 'learning_rate': 1.75e-05, 'epoch': 2.3}\n",
      "{'loss': 0.1783, 'grad_norm': 0.022337140515446663, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0021, 'grad_norm': 0.04051833227276802, 'learning_rate': 1.65e-05, 'epoch': 2.34}\n",
      "{'loss': 0.1309, 'grad_norm': 0.049391187727451324, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.36}\n",
      "{'loss': 0.046, 'grad_norm': 0.036178573966026306, 'learning_rate': 1.55e-05, 'epoch': 2.38}\n",
      "{'loss': 0.2018, 'grad_norm': 9.228836059570312, 'learning_rate': 1.5e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0424, 'grad_norm': 0.03762068226933479, 'learning_rate': 1.45e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0643, 'grad_norm': 0.03821510821580887, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.44}\n",
      "{'loss': 0.0016, 'grad_norm': 0.0509667731821537, 'learning_rate': 1.3500000000000001e-05, 'epoch': 2.46}\n",
      "{'loss': 0.2468, 'grad_norm': 19.036670684814453, 'learning_rate': 1.3000000000000001e-05, 'epoch': 2.48}\n",
      "{'loss': 0.0618, 'grad_norm': 0.052749115973711014, 'learning_rate': 1.25e-05, 'epoch': 2.5}\n",
      "{'loss': 0.0013, 'grad_norm': 0.09354238212108612, 'learning_rate': 1.2e-05, 'epoch': 2.52}\n",
      "{'loss': 0.0172, 'grad_norm': 0.02324100397527218, 'learning_rate': 1.1500000000000002e-05, 'epoch': 2.54}\n",
      "{'loss': 0.1762, 'grad_norm': 78.73534393310547, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.56}\n",
      "{'loss': 0.0025, 'grad_norm': 0.0250403992831707, 'learning_rate': 1.05e-05, 'epoch': 2.58}\n",
      "{'loss': 0.1346, 'grad_norm': 0.05307401344180107, 'learning_rate': 1e-05, 'epoch': 2.6}\n",
      "{'loss': 0.0009, 'grad_norm': 0.04954870790243149, 'learning_rate': 9.5e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0556, 'grad_norm': 0.052427493035793304, 'learning_rate': 9e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0014, 'grad_norm': 0.6030898094177246, 'learning_rate': 8.500000000000002e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0012, 'grad_norm': 0.014631658792495728, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0008, 'grad_norm': 0.017674889415502548, 'learning_rate': 7.5e-06, 'epoch': 2.7}\n",
      "{'loss': 0.096, 'grad_norm': 13.391874313354492, 'learning_rate': 7.000000000000001e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0008, 'grad_norm': 0.0177154503762722, 'learning_rate': 6.5000000000000004e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0293, 'grad_norm': 40.690834045410156, 'learning_rate': 6e-06, 'epoch': 2.76}\n",
      "{'loss': 0.1624, 'grad_norm': 0.016965413466095924, 'learning_rate': 5.500000000000001e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0016, 'grad_norm': 0.0794849693775177, 'learning_rate': 5e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0263, 'grad_norm': 67.09564971923828, 'learning_rate': 4.5e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0007, 'grad_norm': 0.019820112735033035, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0709, 'grad_norm': 80.23101043701172, 'learning_rate': 3.5000000000000004e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0673, 'grad_norm': 1.7760506868362427, 'learning_rate': 3e-06, 'epoch': 2.88}\n",
      "{'loss': 0.1367, 'grad_norm': 0.022476783022284508, 'learning_rate': 2.5e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0009, 'grad_norm': 0.048451319336891174, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0158, 'grad_norm': 0.019780335947871208, 'learning_rate': 1.5e-06, 'epoch': 2.94}\n",
      "{'loss': 0.001, 'grad_norm': 0.02335827425122261, 'learning_rate': 1.0000000000000002e-06, 'epoch': 2.96}\n",
      "{'loss': 0.101, 'grad_norm': 0.0182274729013443, 'learning_rate': 5.000000000000001e-07, 'epoch': 2.98}\n",
      "{'loss': 0.0762, 'grad_norm': 0.039249204099178314, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65c5408775f442a998bf9ef50d494a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.348976969718933, 'eval_runtime': 9.2387, 'eval_samples_per_second': 118.956, 'eval_steps_per_second': 14.937, 'epoch': 3.0}\n",
      "{'train_runtime': 286.2511, 'train_samples_per_second': 41.921, 'train_steps_per_second': 5.24, 'train_loss': 0.24973365316602092, 'epoch': 3.0}\n",
      "Training Time: 286.6905 seconds\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",      \n",
    "    num_train_epochs=3,          \n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=8,  \n",
    "    warmup_steps=500,            \n",
    "    weight_decay=0.01,           \n",
    "    logging_dir=\"./logs\",        \n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "bert = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "start_tm=time.time()\n",
    "bert.train()\n",
    "finish_tm=time.time()\n",
    "training_time = finish_tm - start_tm\n",
    "\n",
    "print(f\"Training Time: {training_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./saved_bert_model\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "save_path = \"./saved_bert_model\"\n",
    "bert.save_model(save_path)\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c23f454e074821be93292736f5dd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67bbd95d50a44bd9e9e4335577cb931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added new result for model: BERT  \n",
      "added new result for model: BERT  \n",
      "added new time_result for model: BERT  \n"
     ]
    }
   ],
   "source": [
    "model_name='BERT '\n",
    "\n",
    "start_tm=time.time()\n",
    "prediction_train=bert.predict(train_dataset)\n",
    "y_pred_train=np.argmax(prediction_train.predictions, axis=1)\n",
    "finish_tm=time.time()\n",
    "pred_train_time=finish_tm - start_tm\n",
    "\n",
    "\n",
    "prediction_test=bert.predict(test_dataset)\n",
    "y_pred_test= np.argmax(prediction_test.predictions, axis=1)\n",
    "\n",
    "#input to eval table\n",
    "eval_model_df_testing = input_model_result(eval_model_df_testing, y_test, y_pred_test,model_name=model_name)\n",
    "eval_model_df_training= input_model_result(eval_model_df_training, y_train, y_pred_train,model_name=model_name)\n",
    "eval_model_time=input_train_test_time(eval_model_time,training_time,pred_train_time,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Model                    | Confusion Matrix          |   TP |   FP |   FN |   TN |   Precision |   Recall |   F1-Score |\n",
      "|---:|:-------------------------|:--------------------------|-----:|-----:|-----:|-----:|------------:|---------:|-----------:|\n",
      "|  0 | K-Nearest Neighbor (kNN) | [[1967, 33], [1232, 768]] |  768 |   33 | 1232 | 1967 |    0.958801 |   0.384  |   0.548376 |\n",
      "|  1 | Naive Bayes              | [[1968, 32], [73, 1927]]  | 1927 |   32 |   73 | 1968 |    0.983665 |   0.9635 |   0.973478 |\n",
      "|  2 | Decision Tree            | [[1961, 39], [11, 1989]]  | 1989 |   39 |   11 | 1961 |    0.980769 |   0.9945 |   0.987587 |\n",
      "|  3 | BERT                     | [[1993, 7], [16, 1984]]   | 1984 |    7 |   16 | 1993 |    0.996484 |   0.992  |   0.994237 |\n",
      "|  4 | SVM                      | [[1961, 39], [13, 1987]]  | 1987 |   39 |   13 | 1961 |    0.98075  |   0.9935 |   0.987084 |\n"
     ]
    }
   ],
   "source": [
    "print(eval_model_df_training.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Model                    | Confusion Matrix       |   TP |   FP |   FN |   TN |   Precision |   Recall |   F1-Score |\n",
      "|---:|:-------------------------|:-----------------------|-----:|-----:|-----:|-----:|------------:|---------:|-----------:|\n",
      "|  0 | K-Nearest Neighbor (kNN) | [[498, 2], [294, 305]] |  305 |    2 |  294 |  498 |    0.993485 | 0.509182 |   0.673289 |\n",
      "|  1 | Naive Bayes              | [[500, 0], [66, 533]]  |  533 |    0 |   66 |  500 |    1        | 0.889816 |   0.941696 |\n",
      "|  2 | Decision Tree            | [[496, 4], [45, 554]]  |  554 |    4 |   45 |  496 |    0.992832 | 0.924875 |   0.957649 |\n",
      "|  3 | BERT                     | [[267, 233], [8, 591]] |  591 |  233 |    8 |  267 |    0.717233 | 0.986644 |   0.830639 |\n",
      "|  4 | SVM                      | [[498, 2], [29, 570]]  |  570 |    2 |   29 |  498 |    0.996503 | 0.951586 |   0.973527 |\n"
     ]
    }
   ],
   "source": [
    "print(eval_model_df_testing.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Model                    |   Training_time |   Predict_time |\n",
      "|---:|:-------------------------|----------------:|---------------:|\n",
      "|  0 | K-Nearest Neighbor (kNN) |      1.27896    |     0.163998   |\n",
      "|  1 | Naive Bayes              |      0.00200081 |     0.00100636 |\n",
      "|  2 | Decision Tree            |      0.324775   |     0.00100446 |\n",
      "|  3 | BERT                     |    286.69       |    17.757      |\n",
      "|  4 | SVM                      |      9.72459    |     0.0090909  |\n"
     ]
    }
   ],
   "source": [
    "print(eval_model_time.to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
