{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 200 persons.\n",
      "Processed Kassie_DePaiva: Train=20, Test=5\n",
      "Processed Bradley_Cooper: Train=20, Test=5\n",
      "Processed Cam_Gigandet: Train=20, Test=5\n",
      "Processed Audra_McDonald: Train=20, Test=5\n",
      "Processed Shannon_Kane: Train=20, Test=5\n",
      "Processed Crystal_Chappell: Train=20, Test=5\n",
      "Processed Jennette_McCurdy: Train=20, Test=5\n",
      "Processed John_Malkovich: Train=20, Test=5\n",
      "Processed Seth_Rogen: Train=20, Test=5\n",
      "Processed Kris_Kristofferson: Train=20, Test=5\n",
      "Processed John_Noble: Train=20, Test=5\n",
      "Processed Catherine_Bell: Train=20, Test=5\n",
      "Processed Sean_Bean: Train=20, Test=5\n",
      "Processed Alyssa_Milano: Train=20, Test=5\n",
      "Processed Jason_Behr: Train=20, Test=5\n",
      "Processed Martin_Sheen: Train=20, Test=5\n",
      "Processed Antonio_Banderas: Train=20, Test=5\n",
      "Processed Tempestt_Bledsoe: Train=20, Test=5\n",
      "Processed Ed_Harris: Train=20, Test=5\n",
      "Processed Valerie_Harper: Train=20, Test=5\n",
      "Processed Amaury_Nolasco: Train=20, Test=5\n",
      "Processed Oliver_Platt: Train=20, Test=5\n",
      "Processed Chyler_Leigh: Train=20, Test=5\n",
      "Processed Jensen_Ackles: Train=20, Test=5\n",
      "Processed Julie_Marie_Berman: Train=20, Test=5\n",
      "Processed Channing_Tatum: Train=20, Test=5\n",
      "Processed David_Boreanaz: Train=20, Test=5\n",
      "Processed Matthew_Lillard: Train=20, Test=5\n",
      "Processed Tia_Carrere: Train=20, Test=5\n",
      "Processed Tamala_Jones: Train=20, Test=5\n",
      "Processed Kim_Delaney: Train=20, Test=5\n",
      "Processed Marilu_Henner: Train=20, Test=5\n",
      "Processed Kimberlin_Brown: Train=20, Test=5\n",
      "Processed Kerr_Smith: Train=20, Test=5\n",
      "Processed Chris_Noth: Train=20, Test=5\n",
      "Processed Pamela_Sue_Martin: Train=20, Test=5\n",
      "Processed Sharon_Gless: Train=20, Test=5\n",
      "Processed Owen_Wilson: Train=20, Test=5\n",
      "Processed James_McAvoy: Train=20, Test=5\n",
      "Processed Victoria_Justice: Train=20, Test=5\n",
      "Processed Clint_Eastwood: Train=20, Test=5\n",
      "Processed Ryan_Gosling: Train=20, Test=5\n",
      "Processed Joanna_Kerns: Train=20, Test=5\n",
      "Processed Sarah_Drew: Train=20, Test=5\n",
      "Processed Julia_Louis-Dreyfus: Train=20, Test=5\n",
      "Processed Ryan_Phillippe: Train=20, Test=5\n",
      "Processed Kathy_Baker: Train=20, Test=5\n",
      "Processed Jonathan_Rhys_Meyers: Train=20, Test=5\n",
      "Processed Samuel_L._Jackson: Train=20, Test=5\n",
      "Processed Yasmine_Bleeth: Train=20, Test=5\n",
      "Processed Vanessa_Marcil: Train=20, Test=5\n",
      "Processed Peggy_Lipton: Train=20, Test=5\n",
      "Processed Amy_Davidson: Train=20, Test=5\n",
      "Processed Bill_Cosby: Train=20, Test=5\n",
      "Processed Shelley_Hennig: Train=20, Test=5\n",
      "Processed Ian_McKellen: Train=20, Test=5\n",
      "Processed January_Jones: Train=20, Test=5\n",
      "Processed Ellen_DeGeneres: Train=20, Test=5\n",
      "Processed Cheryl_Ladd: Train=20, Test=5\n",
      "Processed Kristen_Johnston: Train=20, Test=5\n",
      "Processed Lesley-Anne_Down: Train=20, Test=5\n",
      "Processed Andrea_Bogart: Train=20, Test=5\n",
      "Processed Barbara_Carrera: Train=20, Test=5\n",
      "Processed Jimmy_Fallon: Train=20, Test=5\n",
      "Processed Jean-Claude_Van_Damme: Train=20, Test=5\n",
      "Processed Debi_Mazar: Train=20, Test=5\n",
      "Processed Portia_de_Rossi: Train=20, Test=5\n",
      "Processed Robert_Knepper: Train=20, Test=5\n",
      "Processed Karl_Urban: Train=20, Test=5\n",
      "Processed Matt_Czuchry: Train=20, Test=5\n",
      "Processed Melissa_Archer: Train=20, Test=5\n",
      "Processed Summer_Glau: Train=20, Test=5\n",
      "Processed Ashley_Benson: Train=20, Test=5\n",
      "Processed Jerry_Seinfeld: Train=20, Test=5\n",
      "Processed S._Epatha_Merkerson: Train=20, Test=5\n",
      "Processed America_Ferrera: Train=20, Test=5\n",
      "Processed Matthew_Broderick: Train=20, Test=5\n",
      "Processed Liev_Schreiber: Train=20, Test=5\n",
      "Processed James_Brolin: Train=20, Test=5\n",
      "Processed Joaquin_Phoenix: Train=20, Test=5\n",
      "Processed David_Schwimmer: Train=20, Test=5\n",
      "Processed Candice_Bergen: Train=20, Test=5\n",
      "Processed Ellen_Greene: Train=20, Test=5\n",
      "Processed Laurie_Metcalf: Train=20, Test=5\n",
      "Processed Kiefer_Sutherland: Train=20, Test=5\n",
      "Processed Rachel_Griffiths: Train=20, Test=5\n",
      "Processed Kevin_McKidd: Train=20, Test=5\n",
      "Processed Casey_Affleck: Train=20, Test=5\n",
      "Processed Bonnie_Franklin: Train=20, Test=5\n",
      "Processed Matt_Long: Train=20, Test=5\n",
      "Processed Rebecca_Herbst: Train=20, Test=5\n",
      "Processed Jane_Curtin: Train=20, Test=5\n",
      "Processed Matthew_Perry: Train=20, Test=5\n",
      "Processed Allison_Janney: Train=20, Test=5\n",
      "Processed Colin_Farrell: Train=20, Test=5\n",
      "Processed Mel_Gibson: Train=20, Test=5\n",
      "Processed Dominic_Monaghan: Train=20, Test=5\n",
      "Processed Staci_Keanan: Train=20, Test=5\n",
      "Processed Katrina_Bowden: Train=20, Test=5\n",
      "Processed Nicolas_Cage: Train=20, Test=5\n",
      "Processed Jonathan_Sadowski: Train=20, Test=5\n",
      "Processed Matt_Damon: Train=20, Test=5\n",
      "Processed Dana_Delany: Train=20, Test=5\n",
      "Processed Christopher_Reeve: Train=20, Test=5\n",
      "Processed Julianna_Margulies: Train=20, Test=5\n",
      "Processed Florencia_Lozano: Train=20, Test=5\n",
      "Processed Jackee_Harry: Train=20, Test=5\n",
      "Processed Matthew_Gray_Gubler: Train=20, Test=5\n",
      "Processed Angell_Conwell: Train=20, Test=5\n",
      "Processed Tracey_Gold: Train=20, Test=5\n",
      "Processed Al_Pacino: Train=20, Test=5\n",
      "Processed Chris_Kattan: Train=20, Test=5\n",
      "Skipping Adrianne_LeoÃÅn, not enough images.\n",
      "Processed Joe_Pantoliano: Train=20, Test=5\n",
      "Processed Farah_Fath: Train=20, Test=5\n",
      "Processed Bill_Hader: Train=20, Test=5\n",
      "Processed Dustin_Hoffman: Train=20, Test=5\n",
      "Processed Melissa_Fumero: Train=20, Test=5\n",
      "Processed Carolyn_Hennesy: Train=20, Test=5\n",
      "Processed Patricia_Arquette: Train=20, Test=5\n",
      "Processed Orlando_Bloom: Train=20, Test=5\n",
      "Processed Lacey_Chabert: Train=20, Test=5\n",
      "Processed Robert_Patrick: Train=20, Test=5\n",
      "Processed Geena_Davis: Train=20, Test=5\n",
      "Processed Ray_Stevenson: Train=20, Test=5\n",
      "Processed George_Lopez: Train=20, Test=5\n",
      "Processed Morena_Baccarin: Train=20, Test=5\n",
      "Processed John_Cusack: Train=20, Test=5\n",
      "Processed Jude_Law: Train=20, Test=5\n",
      "Processed Eliza_Coupe: Train=20, Test=5\n",
      "Processed Patricia_Kalember: Train=20, Test=5\n",
      "Processed Susan_Dey: Train=20, Test=5\n",
      "Processed Lauren_Holly: Train=20, Test=5\n",
      "Processed Scott_Patterson: Train=20, Test=5\n",
      "Processed Luke_Wilson: Train=20, Test=5\n",
      "Processed Adrienne_Frantz: Train=20, Test=5\n",
      "Processed Christine_Lakin: Train=20, Test=5\n",
      "Processed Dean_Cain: Train=20, Test=5\n",
      "Processed Marcia_Cross: Train=20, Test=5\n",
      "Processed Carla_Gallo: Train=20, Test=5\n",
      "Processed Chris_Evans: Train=20, Test=5\n",
      "Processed Alec_Baldwin: Train=20, Test=5\n",
      "Processed Jackie_Chan: Train=20, Test=5\n",
      "Processed Richard_Madden: Train=20, Test=5\n",
      "Processed Richard_E._Grant: Train=20, Test=5\n",
      "Processed Omid_Djalili: Train=20, Test=5\n",
      "Processed Laura_Leighton: Train=20, Test=5\n",
      "Processed John_Krasinski: Train=20, Test=5\n",
      "Processed Kevin_Connolly: Train=20, Test=5\n",
      "Processed Jim_Carrey: Train=20, Test=5\n",
      "Processed Ken_Watanabe: Train=20, Test=5\n",
      "Processed Tatyana_M._Ali: Train=20, Test=5\n",
      "Processed Eliza_Dushku: Train=20, Test=5\n",
      "Processed Peter_Sarsgaard: Train=20, Test=5\n",
      "Processed Kristin_Chenoweth: Train=20, Test=5\n",
      "Processed Christopher_Lloyd: Train=20, Test=5\n",
      "Processed Dermot_Mulroney: Train=20, Test=5\n",
      "Processed Sasha_Alexander: Train=20, Test=5\n",
      "Processed Michael_Vartan: Train=20, Test=5\n",
      "Processed Robin_Williams: Train=20, Test=5\n",
      "Processed Roseanne_Barr: Train=20, Test=5\n",
      "Processed Brad_Pitt: Train=20, Test=5\n",
      "Processed Matt_Dillon: Train=20, Test=5\n",
      "Processed Jon_Voight: Train=20, Test=5\n",
      "Processed Natalie_Martinez: Train=20, Test=5\n",
      "Processed Leonardo_DiCaprio: Train=20, Test=5\n",
      "Processed Sherilyn_Fenn: Train=20, Test=5\n",
      "Processed Selena_Gomez: Train=20, Test=5\n",
      "Processed Julie_Bowen: Train=20, Test=5\n",
      "Processed Portia_Doubleday: Train=20, Test=5\n",
      "Processed Jamie_Lee_Curtis: Train=20, Test=5\n",
      "Processed Jared_Padalecki: Train=20, Test=5\n",
      "Processed Alan_Alda: Train=20, Test=5\n",
      "Processed Veronica_Hamel: Train=20, Test=5\n",
      "Processed Josh_Brolin: Train=20, Test=5\n",
      "Processed Clive_Owen: Train=20, Test=5\n",
      "Processed Didi_Conn: Train=20, Test=5\n",
      "Processed Danny_Masterson: Train=20, Test=5\n",
      "Processed Dwayne_Johnson: Train=20, Test=5\n",
      "Processed Anthony_Stewart_Head: Train=20, Test=5\n",
      "Processed Jon_Hamm: Train=20, Test=5\n",
      "Processed Aisha_Hinds: Train=20, Test=5\n",
      "Processed Kaley_Cuoco: Train=20, Test=5\n",
      "Processed Ioan_Gruffudd: Train=20, Test=5\n",
      "Processed Milo_Ventimiglia: Train=20, Test=5\n",
      "Processed Norman_Reedus: Train=20, Test=5\n",
      "Processed Lara_Flynn_Boyle: Train=20, Test=5\n",
      "Processed Arnold_Schwarzenegger: Train=20, Test=5\n",
      "Processed Justine_Bateman: Train=20, Test=5\n",
      "Processed Robert_Redford: Train=20, Test=5\n",
      "Processed Rue_McClanahan: Train=20, Test=5\n",
      "Processed Hector_Elizondo: Train=20, Test=5\n",
      "Processed J.K._Simmons: Train=20, Test=5\n",
      "Processed Chris_Rock: Train=20, Test=5\n",
      "Processed Adrienne_Barbeau: Train=20, Test=5\n",
      "Processed Jake_Gyllenhaal: Train=20, Test=5\n",
      "Processed Audrey_Landers: Train=20, Test=5\n",
      "Processed Jane_Krakowski: Train=20, Test=5\n",
      "Processed Andrea_Anders: Train=20, Test=5\n",
      "Processed Christian_Slater: Train=20, Test=5\n",
      "‚úÖ Dataset organized successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "base_folders = [\"dataset/actor_faces\", \"dataset/actress_faces\"]\n",
    "train_folder = \"dataset/stagging/sampleset\"\n",
    "test_folder = \"dataset/stagging/testingset\"\n",
    "\n",
    "# Function to clear and recreate a folder\n",
    "def reset_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)  # Delete everything inside\n",
    "    os.makedirs(folder_path)  # Recreate the empty folder\n",
    "\n",
    "# Reset train and test folders\n",
    "reset_folder(train_folder)\n",
    "reset_folder(test_folder)\n",
    "# Collect all person folders from both `actor_faces` and `actress_faces`\n",
    "all_persons = []\n",
    "for base_folder in base_folders:\n",
    "    if os.path.exists(base_folder):  # Ensure the folder exists before listing\n",
    "        persons = [os.path.join(base_folder, p) for p in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, p))]\n",
    "        all_persons.extend(persons)  # Add full paths\n",
    "\n",
    "#print(all_persons)\n",
    "# Randomly select up to 200 persons\n",
    "selected_persons = random.sample(all_persons, min(200, len(all_persons)))\n",
    "print(f\"Selected {len(selected_persons)} persons.\")\n",
    "\n",
    "# Process each selected person\n",
    "for person_path in selected_persons:\n",
    "    person_name = os.path.basename(person_path)  # Extract only folder name\n",
    "\n",
    "    images = [img for img in os.listdir(person_path) if img.endswith((\".jpeg\", \".jpg\", \".png\"))]\n",
    "\n",
    "    if len(images) < 20:  # Skip if not enough images\n",
    "        print(f\"Skipping {person_name}, not enough images.\")\n",
    "        continue\n",
    "\n",
    "    # Shuffle and select max 25 images\n",
    "    random.shuffle(images)\n",
    "    selected_images = images[:25]\n",
    "\n",
    "    # Split into train (20) and test (5)\n",
    "    train_images = selected_images[:20]\n",
    "    test_images = selected_images[20:]\n",
    "\n",
    "    # Create person folders in train and test\n",
    "    train_person_path = os.path.join(train_folder, person_name)\n",
    "    test_person_path = os.path.join(test_folder, person_name)\n",
    "    os.makedirs(train_person_path, exist_ok=True)\n",
    "    os.makedirs(test_person_path, exist_ok=True)\n",
    "\n",
    "    # Move images\n",
    "    for img in train_images:\n",
    "        shutil.copy(os.path.join(person_path, img), os.path.join(train_person_path, img))\n",
    "\n",
    "    for img in test_images:\n",
    "        shutil.copy(os.path.join(person_path, img), os.path.join(test_person_path, img))\n",
    "\n",
    "    print(f\"Processed {person_name}: Train={len(train_images)}, Test={len(test_images)}\")\n",
    "\n",
    "print(\"‚úÖ Dataset organized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from mtcnn import MTCNN\n",
    "from imutils import paths\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Najwa\\AppData\\Local\\Temp\\ipykernel_11544\\3602070473.py:3: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_mtcnn(detector,dirpath, dirdest):\n",
    "    \n",
    "    imagePaths = sorted(list(paths.list_images(dirpath)))\n",
    "    for imagePath in tqdm(imagePaths):\n",
    "        #path_split = imagePath.split(os.sep)\n",
    "        path_split=Path(imagePath).parts\n",
    "        name_person = path_split[-2]\n",
    "        fn = path_split[-1].split('.')\n",
    "        filename, fileformat = fn[0], fn[1]\n",
    "\n",
    "        os.makedirs(dirdest, exist_ok=True)\n",
    "\n",
    "        image = cv2.cvtColor(cv2.imread(imagePath), cv2.COLOR_BGR2RGB)\n",
    "        result = detector.detect_faces(image)\n",
    "        print(f\"Processing: {imagePath}, Faces detected: {len(result)}\")\n",
    "\n",
    "        for i in range(len(result)):\n",
    "            bounding_box = result[i]['box']\n",
    "            keypoints = result[i]['keypoints']\n",
    "\n",
    "            bounding_box[0] = max(0, bounding_box[0])\n",
    "            bounding_box[1] = max(0, bounding_box[1])\n",
    "\n",
    "            person_dir = os.path.join(dirdest, name_person)\n",
    "            os.makedirs(person_dir, exist_ok=True)\n",
    "            path_save = os.path.join(person_dir, f\"{filename}_{i}.{fileformat}\")\n",
    "            print(path_save)\n",
    "            img = image[bounding_box[1]:bounding_box[1] + bounding_box[3],\n",
    "                    bounding_box[0]:bounding_box[0] + bounding_box[2]]\n",
    "\n",
    "            cv2.imwrite(path_save, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "    print(\"Face detection and cropping completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/3980 [00:00<18:10,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9044_4231.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9044_4231_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/3980 [00:00<18:02,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9045_4232.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9045_4232_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/3980 [00:00<18:59,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9059_4240.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9059_4240_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9065_4243.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9065_4243_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/3980 [00:01<13:23,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9082_4252.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9082_4252_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9096_4256.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9096_4256_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/3980 [00:01<15:16,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9119_4271.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9119_4271_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9124_4274.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9124_4274_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/3980 [00:02<14:41,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9131_4278.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9131_4278_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/3980 [00:02<17:42,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9149_4286.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9149_4286_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/3980 [00:02<20:04,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9154_4289.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9154_4289_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/3980 [00:03<17:06,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9187_4306.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9187_4306_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9198_4310.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9198_4310_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/3980 [00:03<18:43,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9200_4312.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9200_4312_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/3980 [00:03<18:44,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9213_4318.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9213_4318_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/3980 [00:04<17:41,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9232_4327.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9232_4327_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/3980 [00:04<19:04,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9238_4329.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9238_4329_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9280_4352.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9280_4352_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/3980 [00:05<18:33,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9300_4362.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9300_4362_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/3980 [00:05<17:10,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9341_4378.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Barbeau\\Adrienne_Barbeau_9341_4378_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/3980 [00:05<16:21,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58140_27673.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58140_27673_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58149_27680.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58149_27680_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24/3980 [00:06<14:42,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58157_27688.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58157_27688_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58167_27697.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58167_27697_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 25/3980 [00:06<15:15,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58175_27703.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58175_27703_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/3980 [00:06<15:45,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58177_27704.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58177_27704_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 27/3980 [00:07<18:11,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58185_27711.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58185_27711_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/3980 [00:07<19:26,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58192_27716.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58192_27716_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 29/3980 [00:07<23:11,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58200_27721.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58200_27721_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30/3980 [00:08<21:24,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58213_27729.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58213_27729_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 32/3980 [00:08<18:11,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58218_27733.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58218_27733_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58242_27754.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58242_27754_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 33/3980 [00:08<18:01,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58249_27760.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58249_27760_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58259_27767.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58259_27767_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 35/3980 [00:09<15:07,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58280_27784.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58280_27784_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 36/3980 [00:09<14:55,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58329_27821.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58329_27821_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 37/3980 [00:09<15:06,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58338_27829.png, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58338_27829_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 38/3980 [00:10<18:52,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58347_27835.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58347_27835_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 39/3980 [00:10<19:36,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58377_27858.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58377_27858_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 40/3980 [00:10<18:59,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58380_27861.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Adrienne_Frantz\\Adrienne_Frantz_58380_27861_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78813_35707.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78813_35707_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 42/3980 [00:11<21:09,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78815_35709.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78815_35709_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 44/3980 [00:12<19:44,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78819_35712.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78819_35712_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78831_35719.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78831_35719_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 45/3980 [00:12<17:14,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78833_35721.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78833_35721_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 46/3980 [00:12<18:31,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78837_35723.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78837_35723_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 47/3980 [00:13<23:14,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78843_35726.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78843_35726_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 48/3980 [00:13<28:48,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78848_35727.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78848_35727_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 49/3980 [00:14<27:18,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78851_35729.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78851_35729_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 50/3980 [00:14<25:42,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78853_35731.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78853_35731_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 51/3980 [00:15<31:49,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78858_35734.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78858_35734_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 52/3980 [00:15<28:56,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78890_35746.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78890_35746_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 53/3980 [00:15<24:38,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78894_35747.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78894_35747_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 54/3980 [00:16<27:39,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78927_35758.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78927_35758_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 55/3980 [00:16<26:02,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78934_35761.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78934_35761_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 56/3980 [00:16<22:25,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78952_35771.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78952_35771_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 57/3980 [00:17<22:05,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78977_35776.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78977_35776_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 58/3980 [00:17<19:40,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_78995_35784.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_78995_35784_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 59/3980 [00:17<18:07,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_79041_35796.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_79041_35796_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Aisha_Hinds\\Aisha_Hinds_79077_35803.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Aisha_Hinds\\Aisha_Hinds_79077_35803_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 61/3980 [00:18<29:20,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_1800_964.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_1800_964_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 62/3980 [00:19<38:22,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_1814_975.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_1814_975_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 63/3980 [00:20<39:38,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_1844_999.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_1844_999_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 64/3980 [00:20<42:36,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_1852_1007.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_1852_1007_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 65/3980 [00:21<41:54,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_1855_1008.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_1855_1008_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 66/3980 [00:22<37:58,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_1856_1009.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_1856_1009_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 67/3980 [00:22<35:54,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_1859_1012.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_1859_1012_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 68/3980 [00:23<38:01,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_1879_1027.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_1879_1027_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 69/3980 [00:23<36:45,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_1940_1062.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_1940_1062_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 70/3980 [00:24<34:26,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_1952_1070.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_1952_1070_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 71/3980 [00:24<35:19,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_1970_1076.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_1970_1076_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 72/3980 [00:25<37:24,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_1992_1087.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_1992_1087_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 73/3980 [00:26<45:45,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_2015_1101.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_2015_1101_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 74/3980 [00:27<48:39,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_2034_1112.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_2034_1112_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 75/3980 [00:27<45:02,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_2049_1120.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_2049_1120_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 76/3980 [00:28<43:51,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_2095_1139.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_2095_1139_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 77/3980 [00:29<43:13,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_2100_1141.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_2100_1141_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 78/3980 [00:29<39:04,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_2111_1146.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_2111_1146_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 79/3980 [00:30<37:41,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_2121_1151.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_2121_1151_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 80/3980 [00:30<40:11,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Al_Pacino\\Al_Pacino_2122_1152.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Al_Pacino\\Al_Pacino_2122_1152_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 81/3980 [00:31<40:26,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2165_1180.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2165_1180_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 82/3980 [00:31<36:22,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2192_1204.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2192_1204_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 83/3980 [00:32<37:52,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2206_1215.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2206_1215_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 84/3980 [00:32<36:27,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2207_1216.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2207_1216_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 85/3980 [00:33<36:54,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2218_1222.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2218_1222_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 86/3980 [00:34<37:21,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2224_1228.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2224_1228_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 87/3980 [00:34<35:22,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2233_1235.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2233_1235_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 88/3980 [00:35<33:36,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2256_1251.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2256_1251_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 89/3980 [00:35<34:45,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2259_1253.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2259_1253_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 90/3980 [00:36<32:06,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2297_1279.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2297_1279_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 91/3980 [00:36<30:16,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2308_1287.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2308_1287_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 92/3980 [00:37<33:47,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2321_1298.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2321_1298_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 93/3980 [00:37<33:33,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2323_1300.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2323_1300_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 94/3980 [00:38<38:03,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2352_1323.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2352_1323_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 95/3980 [00:38<38:37,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2385_1345.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2385_1345_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 96/3980 [00:39<36:04,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2389_1349.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2389_1349_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 97/3980 [00:39<33:46,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2396_1354.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2396_1354_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 98/3980 [00:40<35:07,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2442_1388.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2442_1388_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 99/3980 [00:40<32:20,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2451_1395.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2451_1395_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 100/3980 [00:41<34:26,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alan_Alda\\Alan_Alda_2490_1420.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alan_Alda\\Alan_Alda_2490_1420_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 101/3980 [00:42<37:33,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3219_1870.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3219_1870_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 102/3980 [00:43<53:22,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3222_1873.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3222_1873_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 103/3980 [00:44<48:54,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3283_1918.jpeg, Faces detected: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 104/3980 [00:45<52:20,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3296_1931.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3296_1931_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 105/3980 [00:45<46:27,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3314_1942.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3314_1942_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 106/3980 [00:49<1:40:17,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3324_1948.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3324_1948_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 107/3980 [00:49<1:23:52,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3372_1979.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3372_1979_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 108/3980 [00:50<1:07:44,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3437_2029.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3437_2029_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 109/3980 [00:50<58:19,  1.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3449_2040.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3449_2040_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 110/3980 [00:51<56:46,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3456_2045.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3456_2045_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 111/3980 [00:52<56:39,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3467_2054.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3467_2054_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 112/3980 [00:52<47:15,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3478_2064.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3478_2064_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 113/3980 [00:53<48:32,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3483_2067.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3483_2067_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 114/3980 [00:54<46:20,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3494_2077.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3494_2077_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 115/3980 [00:54<40:56,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3500_2082.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3500_2082_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 116/3980 [00:55<40:34,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3533_2108.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3533_2108_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 117/3980 [00:55<36:13,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3545_2116.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3545_2116_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 118/3980 [00:56<34:20,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3550_2119.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3550_2119_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 119/3980 [00:57<38:25,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3551_2120.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3551_2120_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 120/3980 [00:57<34:57,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alec_Baldwin\\Alec_Baldwin_3563_2127.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alec_Baldwin\\Alec_Baldwin_3563_2127_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 121/3980 [00:58<36:13,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_82882_37369.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_82882_37369_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 122/3980 [00:59<44:18,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_82889_37373.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_82889_37373_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 123/3980 [00:59<47:58,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_82913_37390.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_82913_37390_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 124/3980 [01:00<49:14,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_82917_37393.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_82917_37393_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 125/3980 [01:01<50:12,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_82930_37402.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_82930_37402_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 126/3980 [01:02<54:03,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_82931_37403.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_82931_37403_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 127/3980 [01:03<1:01:15,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_82948_37415.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_82948_37415_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 128/3980 [01:04<59:13,  1.08it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_82956_37419.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_82956_37419_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 129/3980 [01:05<51:33,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_82970_37426.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_82970_37426_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 130/3980 [01:05<44:17,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_82980_37432.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_82980_37432_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 131/3980 [01:06<43:42,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_82996_37445.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_82996_37445_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 132/3980 [01:06<39:19,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_83013_37454.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_83013_37454_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 133/3980 [01:07<38:16,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_83016_37457.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_83016_37457_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 134/3980 [01:07<31:58,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_83058_37490.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_83058_37490_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 136/3980 [01:07<22:12,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_83059_37491.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_83059_37491_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_83085_37510.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_83085_37510_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 137/3980 [01:08<20:16,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_83150_37551.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_83150_37551_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 138/3980 [01:08<18:18,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_83168_37561.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_83168_37561_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 139/3980 [01:08<20:07,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_83177_37567.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_83177_37567_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 140/3980 [01:09<21:03,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Allison_Janney\\Allison_Janney_83189_37575.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Allison_Janney\\Allison_Janney_83189_37575_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 141/3980 [01:09<25:09,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_113918_51500.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_113918_51500_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 142/3980 [01:10<25:54,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_113921_51503.jpeg, Faces detected: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 143/3980 [01:10<24:51,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_113922_51504.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_113922_51504_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 144/3980 [01:11<31:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_113933_51513.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_113933_51513_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 145/3980 [01:11<25:56,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_113935_51514.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_113935_51514_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 146/3980 [01:11<26:37,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_113954_51526.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_113954_51526_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 147/3980 [01:12<26:34,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_113957_51528.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_113957_51528_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 148/3980 [01:12<24:51,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_113973_51539.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_113973_51539_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 149/3980 [01:13<27:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_113977_51543.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_113977_51543_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 150/3980 [01:13<27:09,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_113981_51546.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_113981_51546_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 151/3980 [01:13<25:41,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_113990_51551.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_113990_51551_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 152/3980 [01:14<27:11,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_114022_51569.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_114022_51569_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 153/3980 [01:14<29:41,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_114054_51587.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_114054_51587_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 154/3980 [01:15<30:32,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_114055_51588.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_114055_51588_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 155/3980 [01:16<34:26,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_114090_51606.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_114090_51606_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 156/3980 [01:16<29:33,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_114126_51628.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_114126_51628_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 157/3980 [01:17<36:39,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_114186_51661.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_114186_51661_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 158/3980 [01:17<29:36,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_114224_51689.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_114224_51689_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 159/3980 [01:18<34:13,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_114228_51692.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_114228_51692_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 160/3980 [01:18<28:43,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Alyssa_Milano\\Alyssa_Milano_114253_51707.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Alyssa_Milano\\Alyssa_Milano_114253_51707_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 162/3980 [01:19<23:48,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4650_2608.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4650_2608_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4653_2611.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4653_2611_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 163/3980 [01:19<20:39,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4654_2612.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4654_2612_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 164/3980 [01:19<20:33,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4662_2619.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4662_2619_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 165/3980 [01:19<19:11,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4698_2643.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4698_2643_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 167/3980 [01:20<15:45,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4704_2644.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4704_2644_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4754_2677.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4754_2677_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 168/3980 [01:20<16:34,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4769_2684.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4769_2684_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 169/3980 [01:20<16:50,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4770_2685.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4770_2685_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 170/3980 [01:21<17:38,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4776_2688.png, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4776_2688_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 171/3980 [01:21<18:20,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4795_2702.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4795_2702_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 172/3980 [01:21<16:58,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4804_2709.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4804_2709_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 173/3980 [01:21<16:09,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4807_2712.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4807_2712_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 174/3980 [01:22<17:36,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4812_2713.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4812_2713_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 176/3980 [01:22<15:56,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4841_2728.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4841_2728_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4864_2736.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4864_2736_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 178/3980 [01:22<13:45,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4881_2747.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4881_2747_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4900_2759.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4900_2759_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 179/3980 [01:23<12:41,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4921_2771.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4921_2771_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4953_2786.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amaury_Nolasco\\Amaury_Nolasco_4953_2786_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 182/3980 [01:23<13:59,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54566_25818.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54566_25818_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54606_25851.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54606_25851_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 183/3980 [01:24<13:38,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54607_25852.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54607_25852_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54614_25856.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54614_25856_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 185/3980 [01:24<12:55,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54616_25858.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54616_25858_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54621_25863.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54621_25863_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 187/3980 [01:24<15:11,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54634_25877.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54634_25877_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54642_25883.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54642_25883_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 189/3980 [01:25<14:26,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54644_25885.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54644_25885_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 191/3980 [01:25<14:37,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54662_25901.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54662_25901_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54749_25970.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54749_25970_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 192/3980 [01:26<14:28,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54785_25995.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54785_25995_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54786_25996.jpeg, Faces detected: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 193/3980 [01:26<13:58,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54786_25996_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 194/3980 [01:26<14:26,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54807_26016.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54807_26016_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 195/3980 [01:26<16:49,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54812_26020.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54812_26020_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 196/3980 [01:27<18:06,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54819_26027.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54819_26027_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 197/3980 [01:27<18:48,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54843_26042.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54843_26042_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 198/3980 [01:27<18:11,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54849_26047.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54849_26047_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 199/3980 [01:28<19:46,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54859_26052.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54859_26052_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 200/3980 [01:28<20:15,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\America_Ferrera\\America_Ferrera_54916_26095.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\America_Ferrera\\America_Ferrera_54916_26095_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 201/3980 [01:29<30:55,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amy_Davidson\\Amy_Davidson_39563_18454.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amy_Davidson\\Amy_Davidson_39563_18454_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 202/3980 [01:30<35:27,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amy_Davidson\\Amy_Davidson_39571_18460.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amy_Davidson\\Amy_Davidson_39571_18460_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 203/3980 [01:30<30:29,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amy_Davidson\\Amy_Davidson_39574_18463.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amy_Davidson\\Amy_Davidson_39574_18463_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 204/3980 [01:31<31:38,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amy_Davidson\\Amy_Davidson_39583_18471.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amy_Davidson\\Amy_Davidson_39583_18471_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 205/3980 [01:31<26:15,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amy_Davidson\\Amy_Davidson_39597_18482.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amy_Davidson\\Amy_Davidson_39597_18482_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 207/3980 [01:31<20:56,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dataset/stagging/sampleset\\Amy_Davidson\\Amy_Davidson_39598_18483.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amy_Davidson\\Amy_Davidson_39598_18483_0.jpeg\n",
      "Processing: dataset/stagging/sampleset\\Amy_Davidson\\Amy_Davidson_39602_18486.jpeg, Faces detected: 1\n",
      "dataset/pcs_mtcnn_faces/sampleset\\Amy_Davidson\\Amy_Davidson_39602_18486_0.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 207/3980 [01:31<27:54,  2.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m dirpaths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/stagging/sampleset\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/stagging/testingset\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dirpath \u001b[38;5;129;01min\u001b[39;00m dirpaths:\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mprocessed_mtcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMTCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdirpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdirdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstagging\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpcs_mtcnn_faces\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 14\u001b[0m, in \u001b[0;36mprocessed_mtcnn\u001b[1;34m(detector, dirpath, dirdest)\u001b[0m\n\u001b[0;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(dirdest, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(cv2\u001b[38;5;241m.\u001b[39mimread(imagePath), cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 14\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimagePath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Faces detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(result)):\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\mtcnn\\mtcnn.py:159\u001b[0m, in \u001b[0;36mMTCNN.detect_faces\u001b[1;34m(self, image, fit_to_image, limit_boundaries_landmarks, box_format, output_type, postprocess, batch_stack_justification, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Process images through each stage (PNet, RNet, ONet)\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages:\n\u001b[1;32m--> 159\u001b[0m         bboxes_batch \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbboxes_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbboxes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_normalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_oshapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_oshapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mInvalidArgumentError:  \u001b[38;5;66;03m# No faces found\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     bboxes_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m16\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\mtcnn\\stages\\stage_pnet.py:92\u001b[0m, in \u001b[0;36mStagePNet.__call__\u001b[1;34m(self, images_normalized, images_oshapes, min_face_size, min_size, scale_factor, threshold_pnet, nms_pnet1, nms_pnet2, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m images_normalized\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# 3. Get proposals bounding boxes and confidence from the model (PNet)\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m pnet_result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m scales_result]\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# 4. Generate bounding boxes per scale group\u001b[39;00m\n\u001b[0;32m     95\u001b[0m bboxes_proposals \u001b[38;5;241m=\u001b[39m [generate_bounding_box(result[\u001b[38;5;241m0\u001b[39m], result[\u001b[38;5;241m1\u001b[39m], threshold_pnet) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m pnet_result]\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:899\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[0;32m     43\u001b[0m         call_fn,\n\u001b[0;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\mtcnn\\network\\pnet.py:99\u001b[0m, in \u001b[0;36mPNet.call\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Outputs\u001b[39;00m\n\u001b[0;32m     98\u001b[0m bbox_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4_1(x)\n\u001b[1;32m---> 99\u001b[0m bbox_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv4_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [bbox_reg, bbox_class]\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:899\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[0;32m     43\u001b[0m         call_fn,\n\u001b[0;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:252\u001b[0m, in \u001b[0;36mBaseConv.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m         bias_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m--> 252\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m     outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\ops\\numpy.py:4728\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(x, newshape)\u001b[0m\n\u001b[0;32m   4726\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[0;32m   4727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Reshape(newshape)\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[1;32m-> 4728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py:1851\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(x, newshape)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     output\u001b[38;5;241m.\u001b[39mset_shape(output_shape)\n\u001b[0;32m   1850\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m-> 1851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:199\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanip.reshape\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     64\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(tensor, shape, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Reshapes a tensor.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m  Given `tensor`, this operation returns a new `tf.Tensor` that has the same\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    A `Tensor`. Has the same type as `tensor`.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m   shape_util\u001b[38;5;241m.\u001b[39mmaybe_set_static_shape(result, shape)\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:10889\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m  10887\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m  10888\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 10889\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreshape_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10890\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10891\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m  10892\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:10910\u001b[0m, in \u001b[0;36mreshape_eager_fallback\u001b[1;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[0;32m  10909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape_eager_fallback\u001b[39m(tensor: Annotated[Any, TV_Reshape_T], shape: Annotated[Any, TV_Reshape_Tshape], name, ctx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Annotated[Any, TV_Reshape_T]:\n\u001b[1;32m> 10910\u001b[0m   _attr_T, (tensor,) \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_to_matching_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10911\u001b[0m   _attr_Tshape, (shape,) \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39margs_to_matching_eager([shape], ctx, [_dtypes\u001b[38;5;241m.\u001b[39mint32, _dtypes\u001b[38;5;241m.\u001b[39mint64, ], _dtypes\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m  10912\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [tensor, shape]\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:259\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    256\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 259\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m      \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_dtype\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m ret\u001b[38;5;241m.\u001b[39mappend(tensor)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:209\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    207\u001b[0m overload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__tf_tensor__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 209\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moverload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base_type, conversion_func \u001b[38;5;129;01min\u001b[39;00m get(\u001b[38;5;28mtype\u001b[39m(value)):\n\u001b[0;32m    212\u001b[0m   \u001b[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[0;32m    213\u001b[0m   \u001b[38;5;66;03m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:77\u001b[0m, in \u001b[0;36mVariable.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:161\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m\u001b[38;5;241m.\u001b[39mtf_export(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m     97\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[0;32m     99\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tensor_lib\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:171\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m           _add_error_prefix(\n\u001b[0;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:2375\u001b[0m, in \u001b[0;36m_dense_var_to_tensor\u001b[1;34m(var, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m   2374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dense_var_to_tensor\u001b[39m(var, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 2375\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dense_var_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1621\u001b[0m, in \u001b[0;36mBaseResourceVariable._dense_var_to_tensor\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1619\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_value()\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1621\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:656\u001b[0m, in \u001b[0;36mBaseResourceVariable.value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    654\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_value\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28;01mNone\u001b[39;00m, ignore_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 656\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:841\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    839\u001b[0m       result \u001b[38;5;241m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    844\u001b[0m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    845\u001b[0m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    846\u001b[0m   record\u001b[38;5;241m.\u001b[39mrecord_operation(\n\u001b[0;32m    847\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle],\n\u001b[0;32m    848\u001b[0m       backward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    849\u001b[0m       forward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:831\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat\u001b[38;5;241m.\u001b[39mforward_compatible(\u001b[38;5;241m2022\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m    830\u001b[0m   gen_resource_variable_ops\u001b[38;5;241m.\u001b[39mdisable_copy_on_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m--> 831\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, result)\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Najwa\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:590\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    589\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 590\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadVariableOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    593\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dirpaths=['dataset/stagging/sampleset','dataset/stagging/testingset']\n",
    "\n",
    "for dirpath in dirpaths:\n",
    "    processed_mtcnn(detector=MTCNN(),dirpath=dirpath,dirdest=dirpath.replace(\"stagging\",\"pcs_mtcnn_faces\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
